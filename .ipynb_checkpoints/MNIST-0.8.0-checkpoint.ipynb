{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating data in train data and validation data\n",
    "\n",
    "Here, we used stratified train test split sklearn function to separate our data. It's defined only 10% of all data to validation data. Here we have experimented to predict 0-5 handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21416, 784)\n",
      "(2142,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "train_labels = train_data.iloc[:, 0].values\n",
    "boolean_indices = train_labels < 5 \n",
    "train_labels = train_labels[boolean_indices]\n",
    "\n",
    "train_data = train_data.iloc[:, 1:].values\n",
    "train_data = train_data[boolean_indices]\n",
    "train_data = train_data.astype(np.float64)\n",
    "\n",
    "print(train_data.shape)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(train_data, train_labels,\n",
    "                                                                    test_size=0.1, stratify=train_labels)\n",
    "\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, layers=[100, 100, 100, 100, 100], batch_size=128, optimizer_class = tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, activation = tf.nn.elu, dropout_rate=None, momentum_batch=None, random_state=None):\n",
    "        self.layers = layers\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.momentum_batch = momentum_batch\n",
    "        self.batch_size = batch_size\n",
    "        self._session = None\n",
    "        self._training = None\n",
    "        \n",
    "        if random_state is not None:\n",
    "            tf.set_random_seed(random_state)\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "    def _dnn(self, X):\n",
    "        '''Here we build our dnn structure'''\n",
    "        for index, n_layer in enumerate(self.layers):\n",
    "            if self.dropout_rate:\n",
    "                X = tf.layers.dropout(X, self.dropout_rate, training=self._training)\n",
    "\n",
    "            X = tf.layers.dense(X, n_layer)\n",
    "            if self.momentum_batch:\n",
    "                X = tf.layers.batch_normalization(X, training=self._training, momentum=self.momentum_batch)\n",
    "            X = self.activation(X, name=\"hidden%d\" % (index))\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _prepare_dataset(self, X_shape = None, y = None):\n",
    "        \n",
    "        '''Prepare our tensorflow dataset'''\n",
    "        self._batch_size = tf.placeholder(tf.int64, shape=(), name=\"batch_size\")\n",
    "        self._X = tf.placeholder(tf.float64, shape=[None, X_shape[1]], name=\"X\")\n",
    "        self._y = tf.placeholder(tf.int64, shape=[None], name=\"y\")\n",
    "\n",
    "        def map_fn(data, labels):\n",
    "            data = tf.math.divide(data, 255)\n",
    "            return data, labels\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((self._X, self._y))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((self._X, self._y))\n",
    "        \n",
    "        train_dataset = train_dataset.repeat().shuffle(X_shape[0])\n",
    "        \n",
    "        train_dataset = train_dataset.batch(self._batch_size)\n",
    "        train_dataset = train_dataset.map(map_fn, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "        train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        val_dataset = val_dataset.batch(self._batch_size)\n",
    "        val_dataset = val_dataset.map(map_fn, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "        val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        self._handle = tf.placeholder(tf.string, shape = [])\n",
    "        iterator = tf.data.Iterator.from_string_handle(self._handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "        \n",
    "        self._data, self._labels = iterator.get_next()\n",
    "        \n",
    "        self._train_val_iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "        self._train_iterator = self._train_val_iterator.make_initializer(train_dataset)\n",
    "        self._val_iterator = val_dataset.make_initializable_iterator()\n",
    "        \n",
    "\n",
    "    def _build_graph(self, n_outputs):\n",
    "        '''Building tensorflow graph'''\n",
    "        \n",
    "        self._training = tf.placeholder_with_default(True, shape=[], name=\"training\")\n",
    "\n",
    "        with tf.name_scope(\"build\"):\n",
    "\n",
    "            last_hidden_layer = self._dnn(self._data)\n",
    "\n",
    "            logits = tf.layers.dense(last_hidden_layer, n_outputs, name=\"outputs\")\n",
    "            logits = tf.cast(logits, tf.float32)\n",
    "            y_proba = tf.nn.softmax(logits, name=\"y_proba\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self._labels, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "            loss_summary = tf.summary.scalar(\"log_loss\", loss)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = self.optimizer_class(self.learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "\n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, self._labels, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float64), name=\"accuracy\")\n",
    "            accuracy_summary = tf.summary.scalar(\"log_accuracy\", accuracy)\n",
    "\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        self._saver = saver\n",
    "        self._accuracy, self._accuracy_summary = accuracy, accuracy_summary\n",
    "        self._loss, self._loss_summary = loss, loss_summary\n",
    "        self._training_op = training_op\n",
    "        self._y_proba = y_proba\n",
    "        self._init = init\n",
    "\n",
    "    \n",
    "    def _get_model_params(self):\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "            return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "    \n",
    "    def _restore_model_params(self, model_params):\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        \n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict = feed_dict)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, n_epochs=10001, summary=False):\n",
    "\n",
    "        def logdir(prefix=\"\"):\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "            root_dir = \"tf_logs/\"\n",
    "            return root_dir + prefix + \"run-\" + now\n",
    "        \n",
    "        \n",
    "        n_inputs = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_outputs = len(self._classes)\n",
    "\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._prepare_dataset(n_inputs)\n",
    "            self._build_graph(n_outputs)\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            \n",
    "            if summary:\n",
    "                file_writer = tf.summary.FileWriter(logdir(\"MNIST\"), tf.get_default_graph())\n",
    "\n",
    "                \n",
    "        best_loss = np.infty\n",
    "        counter_early_stopping=0\n",
    "        counter_max_value = 200\n",
    "\n",
    "            \n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:                                        \n",
    "            sess.run(self._train_iterator, feed_dict={self._X: X, self._y: y,\n",
    "                                                    self._training: True, self._batch_size : self.batch_size})\n",
    "\n",
    "#                 if os.path.isfile(checkpoint_epoch_path):\n",
    "#                     with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "#                         start_epoch = int(f.read())\n",
    "#                     print(\"Foi interrompido! Parou no {} epoch\".format(start_epoch))\n",
    "#                     saver.restore(sess, checkpoint_path)\n",
    "\n",
    "#                 else:\n",
    "            start_epoch = 0\n",
    "            sess.run(self._init)\n",
    "            train_val_string = sess.run(self._train_val_iterator.string_handle())\n",
    "\n",
    "            feed_dict = {self._handle: train_val_string}\n",
    "            for epoch in range(start_epoch, n_epochs):\n",
    "                _, _loss_summary, _accuracy_summary, loss_value, accuracy_value = sess.run([self._training_op,\n",
    "                                                                                            self._loss_summary,\n",
    "                                                                                            self._accuracy_summary,\n",
    "                                                                                            self._loss, self._accuracy],\n",
    "                                                                                            feed_dict=feed_dict)\n",
    "                \n",
    "                if extra_update_ops:\n",
    "                    sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                \n",
    "                if loss_value < best_loss:\n",
    "                    best_params = self._get_model_params()\n",
    "                    best_loss = loss_value\n",
    "                    counter_early_stopping = 0\n",
    "                    \n",
    "                else:\n",
    "                    counter_early_stopping += 1\n",
    "                    if counter_early_stopping > counter_max_value:\n",
    "                        print(\"Early Stopping!\")\n",
    "                        break\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    if summary:\n",
    "                        file_writer.add_summary(_loss_summary, epoch)\n",
    "                        file_writer.add_summary(_accuracy_summary, epoch)\n",
    "\n",
    "                if epoch % 50 == 0:\n",
    "                    accuracy_value = sess.run(self._accuracy, feed_dict=feed_dict)\n",
    "                    print(\"Best Loss: {:.6f}\\t\\tTrain Score: {:.4f}\".format(best_loss, accuracy_value))\n",
    "\n",
    "            \n",
    "            if best_params:\n",
    "                self._best_params = best_params\n",
    "                self._restore_model_params(self._best_params)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "#fix this part   \n",
    "    def score(self, X, y, method = accuracy_score):\n",
    "        with self._session.as_default() as sess:\n",
    "            \n",
    "            train_val_string = sess.run(self._train_val_iterator.string_handle())\n",
    "            \n",
    "            sess.run(self._train_iterator, feed_dict={self._X: X, self._y: y,\n",
    "                                                    self._training: False, self._batch_size: len(X)})\n",
    "            \n",
    "            _, y_proba, labels_val = sess.run([self._loss, self._y_proba, self._labels],\n",
    "                                              feed_dict = {self._training: False, self._handle: train_val_string})\n",
    "            \n",
    "            y_pred = np.argmax(y_proba, axis = 1)\n",
    "            \n",
    "            return method(labels_val, y_pred)\n",
    "####\n",
    "\n",
    "    def predict(self, X):\n",
    "         with self._session.as_default() as sess:\n",
    "            \n",
    "            test_string = sess.run(self._val_iterator.string_handle())\n",
    "            \n",
    "            sess.run(self._val_iterator.initializer, feed_dict={self._X: X, self._y: np.zeros((len(X))),\n",
    "                                                                self._training: False,\n",
    "                                                                self._batch_size: len(X),\n",
    "                                                                self._handle: test_string})\n",
    "    \n",
    "            y_proba_val = self._y_proba.eval(feed_dict = {self._handle: test_string})\n",
    "        \n",
    "            return np.argmax(y_proba_val, axis=1)\n",
    "        \n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = MNISTClassifier(random_state=42)\n",
    "\n",
    "# mnist.fit(train_data, train_labels)\n",
    "# accuracy = mnist.score(test_data, test_labels)\n",
    "# # precision = mnist.score(test_data, test_labels, method = precision_score)\n",
    "# print(\"Test accuracy score: {}\".format(accuracy))\n",
    "# # print(\"Test precision score: {}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_distribs = {\n",
    "#     \"layers\": [[100, 100, 100, 100, 100], [512, 256, 128, 64, 32], [50, 50, 50, 50, 50], [80, 80, 80, 80, 80]],\n",
    "#     \"batch_size\": [16, 32, 64, 128, 256, 512],\n",
    "#     \"learning_rate\": [0.01, 0.02, 0.05, 0.1, 0.2],\n",
    "#     \"activation\": [tf.nn.relu, tf.nn.elu],\n",
    "#     \"momentum_batch\": [None, 0.9, 0.99, 0.999],\n",
    "#     \"dropout_rate\": [None, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "#     \"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)]\n",
    "# }\n",
    "\n",
    "# clf = RandomizedSearchCV(MNISTClassifier(random_state=42), param_distribs, cv=3, n_iter=350, random_state=42)\n",
    "# clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.980831\t\tTrain Score: 0.7422\n",
      "Best Loss: 0.013515\t\tTrain Score: 0.9805\n",
      "Best Loss: 0.013515\t\tTrain Score: 0.9902\n",
      "Best Loss: 0.006960\t\tTrain Score: 0.9902\n",
      "Best Loss: 0.006960\t\tTrain Score: 0.9922\n",
      "Best Loss: 0.004739\t\tTrain Score: 0.9980\n",
      "Best Loss: 0.002659\t\tTrain Score: 0.9941\n",
      "Best Loss: 0.002659\t\tTrain Score: 0.9980\n",
      "Best Loss: 0.001070\t\tTrain Score: 0.9980\n",
      "Best Loss: 0.001070\t\tTrain Score: 0.9961\n",
      "Best Loss: 0.001070\t\tTrain Score: 1.0000\n",
      "Best Loss: 0.001070\t\tTrain Score: 0.9922\n",
      "Best Loss: 0.000607\t\tTrain Score: 0.9922\n",
      "Best Loss: 0.000456\t\tTrain Score: 1.0000\n",
      "Best Loss: 0.000456\t\tTrain Score: 0.9980\n",
      "Best Loss: 0.000399\t\tTrain Score: 0.9980\n",
      "Best Loss: 0.000399\t\tTrain Score: 0.9961\n",
      "Best Loss: 0.000399\t\tTrain Score: 0.9922\n",
      "Best Loss: 0.000399\t\tTrain Score: 0.9980\n",
      "Early Stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MNISTClassifier(activation=<function relu at 0x00000232819B9400>,\n",
       "        batch_size=512, dropout_rate=0.1, layers=[512, 256, 128, 64, 32],\n",
       "        learning_rate=0.01, momentum_batch=0.9,\n",
       "        optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "        random_state=None)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = MNISTClassifier(momentum_batch=0.9, learning_rate=0.01, layers=[512, 256, 128, 64, 32], dropout_rate=0.1, batch_size=512, random_state=42, activation=tf.nn.relu)\n",
    "mnist.fit(train_data, train_labels, summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9869281045751634\n"
     ]
    }
   ],
   "source": [
    "accuracy = mnist.score(test_data, test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 0 2 1 1 2 0 2 4 4 3 4 1 4 1 4 4 4 2 1 3 2 1 3 0 4 3 3 0 1 2 2 1 2 0 2\n",
      " 1 2 3 3 2 0 4 3 0 4 1 4 4 1 1 2 4 3 2 0 2 3 2]\n",
      "[3 2 0 2 1 1 2 0 2 4 4 3 4 1 4 1 4 4 4 2 1 3 2 1 3 0 4 3 3 0 1 2 2 1 2 0 2\n",
      " 1 2 3 3 2 0 4 3 0 4 1 4 4 1 1 2 4 3 2 0 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "indices = mnist.predict(train_data[0:60])\n",
    "print(train_labels[0:60])\n",
    "print(indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
